---
title: "PTSD Power calculations"
author: "Catherine"
date: "2025-02-01"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## ROC analysis

```{r roc analysis}
# Function to calculate sample size for multi-class ROC analysis with unequal class sizes
# Function to calculate sample size for multi-class ROC analysis with debug info
calc_multiclass_roc_sample_size <- function(power = 0.8, 
                                          alpha = 0.05,
                                          min_auc = 0.7,
                                          null_auc = 0.5,
                                          num_classes = 3,
                                          class_proportions = NULL,
                                          min_observations = 30,
                                          max_sample_size = 1e6) {
  
  # Validate inputs
  if (is.null(class_proportions)) {
    class_proportions <- rep(1/num_classes, num_classes)
  }
  if (length(class_proportions) != num_classes) {
    stop("Number of proportions must match number of classes")
  }
  if (abs(sum(class_proportions) - 1) > 1e-10) {
    stop("Class proportions must sum to 1")
  }
  
  # Modified variance calculation with safeguards
  calc_auc_variance <- function(auc, n1, n2) {
    # Add small constant to prevent division by zero
    n1 <- max(n1, 1)
    n2 <- max(n2, 1)
    
    q1 <- auc / (2 - auc)
    q2 <- 2 * auc^2 / (1 + auc)
    
    variance <- (auc * (1 - auc) + (n1 - 1) * (q1 - auc^2) + 
                (n2 - 1) * (q2 - auc^2)) / (n1 * n2)
    
    # Cap maximum variance to prevent explosion
    return(min(variance, 1.0))
  }
  
  # Calculate z-scores
  z_alpha <- qnorm(1 - alpha/2)
  z_beta <- qnorm(power)
  
  # Initial sample size based on minimum class size
  min_prop <- min(class_proportions)
  n_total <- ceiling(min_observations / min_prop)
  
  # Debug information
  debug_info <- list()
  
  # Iterative process with diagnostics
  for (iteration in 1:100) {
    n_classes <- floor(n_total * class_proportions)
    
    # Store diagnostic information
    debug_info[[iteration]] <- list(
      iteration = iteration,
      n_total = n_total,
      n_classes = n_classes
    )
    
    max_var <- 0
    required_n <- numeric(0)
    
    # Calculate required sample size for each pair
    for (i in 1:(num_classes-1)) {
      for (j in (i+1):num_classes) {
        var_null <- calc_auc_variance(null_auc, n_classes[i], n_classes[j])
        var_alt <- calc_auc_variance(min_auc, n_classes[i], n_classes[j])
        
        n_pair <- ((z_alpha * sqrt(var_null) + z_beta * sqrt(var_alt))^2) / 
                  (min_auc - null_auc)^2
        
        required_n <- c(required_n, n_pair)
        max_var <- max(max_var, var_null, var_alt)
      }
    }
    
    # Calculate new total sample size needed
    new_n_total <- ceiling(max(required_n) / min_prop)
    
    # Add to debug info
    debug_info[[iteration]]$max_variance <- max_var
    debug_info[[iteration]]$required_n <- required_n
    debug_info[[iteration]]$new_n_total <- new_n_total
    
    # Check if sample size is too large
    if (new_n_total > max_sample_size) {
      return(list(
        error = "Sample size exceeded maximum limit",
        last_n_total = new_n_total,
        debug_info = debug_info,
        suggested_solutions = c(
          "Decrease power requirement",
          "Increase minimum AUC difference",
          "Consider addressing class imbalance through sampling",
          "Increase alpha (currently very conservative)"
        )
      ))
    }
    
    # Check convergence
    if (abs(new_n_total - n_total) < 1) {
      final_n_classes <- floor(new_n_total * class_proportions)
      return(list(
        sample_sizes_per_class = final_n_classes,
        total_sample_size = sum(final_n_classes),
        class_proportions = class_proportions,
        power = power,
        alpha = alpha,
        min_auc = min_auc,
        null_auc = null_auc,
        num_classes = num_classes,
        converged = TRUE,
        iterations = iteration,
        debug_info = debug_info
      ))
    }
    
    n_total <- new_n_total
  }
  
  # If we reach here, didn't converge
  return(list(
    error = "Failed to converge",
    debug_info = debug_info,
    last_n_total = n_total
  ))
}


# Example 2: Unequal proportions (e.g., 50%, 30%, 20%)
result2 <- calc_multiclass_roc_sample_size(
  power = 0.8,
  min_auc = 0.7,
  num_classes = 3,
  class_proportions = c(0.05, 0.17, 0.78)
)

result2
```

```{r test}
library(pROC)
roc3 = 0.7
roc4 = 0.7

# power.roc.test(roc3, roc4, power=0.8, method="obuchowski")

# Define parameters for the ROC analysis
auc1 <- 0.7  # AUC for the first cutoff score
auc2 <- 0.7  # AUC for the second cutoff score
alpha <- 0.05  # Significance level
power <- 0.95  # Desired power of the test


n_needed <- power.roc.test(auc = 0.70,
                          power = 0.8,
                          sig.level = 0.05,
                          kappa = 5/17)

n_needed # 105, 31
# 66 20

n_needed <- power.roc.test(auc = 0.70,
                          power = 0.8,
                          sig.level = 0.05,
                          kappa = 78/17)

n_needed # 32, 143
# 19 87


# over all
105 + 31 + 143
66 + 20 + 87



# Perform the power calculation to find the sample size needed
result <- power.roc.test(auc = c(auc1, auc2), 
                         power = power, 
                         alpha = alpha)

result
# The power.roc.test function will calculate and output the minimum sample size required to achieve the specified power for detecting a difference between the two AUCs

power.roc.test(auc=0.73, sig.level=0.05, power=0.95, kappa=1.7)


power.roc.test(roc3, roc4, power=0.8, sig.level=NULL, method="obuchowski")

power.roc.test(roc3, roc4, power=0.9, sig.level=NULL, method="obuchowski")



data(aSAH)

#### One ROC curve ####

# Build a roc object:
rocobj <- roc(aSAH$outcome, aSAH$s100b)

# Determine power of one ROC curve:
power.roc.test(rocobj)
# Same as:
power.roc.test(ncases=41, ncontrols=72, auc=0.73, sig.level=0.05)
# sig.level=0.05 is implicit and can be omitted:
power.roc.test(ncases=41, ncontrols=72, auc=0.73)

# Determine ncases & ncontrols:
power.roc.test(auc=rocobj$auc, sig.level=0.05, power=0.95, kappa=1.7)
power.roc.test(auc=0.73, sig.level=0.05, power=0.95, kappa=1.7)

# Determine sig.level:
power.roc.test(ncases=41, ncontrols=72, auc=0.73, power=0.95, sig.level=NULL)

# Derermine detectable AUC:
power.roc.test(ncases=41, ncontrols=72, sig.level=0.05, power=0.95)


#### Two ROC curves ####

###  Full AUC
roc1 <- roc(aSAH$outcome, aSAH$ndka)
roc2 <- roc(aSAH$outcome, aSAH$wfns)

## Sample size
# With DeLong variance (default)
power.roc.test(roc1, roc2, power=0.9)
# With Obuchowski variance
power.roc.test(roc1, roc2, power=0.9, method="obuchowski")

## Power test
# With DeLong variance (default)
power.roc.test(roc1, roc2)
# With Obuchowski variance
power.roc.test(roc1, roc2, method="obuchowski")

## Significance level
# With DeLong variance (default)
power.roc.test(roc1, roc2, power=0.9, sig.level=NULL)
# With Obuchowski variance
power.roc.test(roc1, roc2, power=0.9, sig.level=NULL, method="obuchowski")

### Partial AUC
roc3 <- roc(aSAH$outcome, aSAH$ndka, partial.auc=c(1, 0.9))
roc4 <- roc(aSAH$outcome, aSAH$wfns, partial.auc=c(1, 0.9))

## Sample size
# With bootstrap variance (default)
## Not run: 
power.roc.test(roc3, roc4, power=0.9)

## End(Not run)
# With Obuchowski variance
power.roc.test(roc3, roc4, power=0.9, method="obuchowski")

## Power test
# With bootstrap variance (default)
## Not run: 
power.roc.test(roc3, roc4)
# This is exactly equivalent:
power.roc.test(roc1, roc2, reuse.auc=FALSE, partial.auc=c(1, 0.9))

## End(Not run)
# With Obuchowski variance
power.roc.test(roc3, roc4, method="obuchowski")

## Significance level
# With bootstrap variance (default)
## Not run: 
power.roc.test(roc3, roc4, power=0.9, sig.level=NULL)

## End(Not run)
# With Obuchowski variance
power.roc.test(roc3, roc4, power=0.9, sig.level=NULL, method="obuchowski")

## With only binormal parameters given
# From example 2 of Obuchowski and McClish, 1997.
ob.params <- list(A1=2.6, B1=1, A2=1.9, B2=1, rn=0.6, ra=0.6, FPR11=0,
FPR12=0.2, FPR21=0, FPR22=0.2, delta=0.037) 

power.roc.test(ob.params, power=0.8, sig.level=0.05)
power.roc.test(ob.params, power=0.8, sig.level=NULL, ncases=107)
power.roc.test(ob.params, power=NULL, sig.level=0.05, ncases=107)


```
## Chi square on G power

## Latent profile analysis
The results were found from table 8, https://pmc.ncbi.nlm.nih.gov/articles/PMC4196274/#S7
The sample size is approximately to the 15 items' case with a conservative estimation: 825*1.15 = 949.9


## Multinomial regression

```{r}
# reference: https://library.virginia.edu/data/articles/simulating-multinomial-logistic-regression-data
library(nnet)
library(car)
sim_mod <- function(n){
  # generate predictors
  x <- runif(n = n, min = 0.5, max = 3)
  g <- sample(c("a", "b"), size = n, replace = TRUE)
  # linear predictors
  lp2 <- 0.74 + 0.74*x + 0.74*(g == "b")
  lp3 <- 0.72 + 0.72*x + 0.72*(g == "b")
  # probabilities
  den <- (1 + exp(lp2) + exp(lp3))
  p1 <- 1/den
  p2 <- exp(lp2)/den
  p3 <- exp(lp3)/den
  P <- cbind(p1, p2, p3)
  rout <- replicate(n = 500, expr = {
    y <- apply(P, MARGIN = 1, function(x)sample(x = 1:3, size = 1, prob = x))
    d <- data.frame(y = factor(y), x, g)
    m <- multinom(y ~ x + g, data = d, trace = FALSE)
    aod <- car::Anova(m)
    aod["g", "Pr(>Chisq)"] < 0.05
    })
  mean(rout)
}

sizes <- sapply(c(1000, 1500, 2000, 2500, 3000), sim_mod)
sizes

#2500

```


```{r same code different OR}

library(nnet)
library(car)

sim_mod <- function(n){
  # generate predictors
  x <- runif(n = n, min = 0.5, max = 3)
  g <- sample(c("a", "b"), size = n, replace = TRUE)
  # linear predictors
  lp2 <- 3 + 3.39*x + 2.39*(g == "b")
  lp3 <- 10.65 + 5*x + 7.56*(g == "b")
  # probabilities
  den <- (1 + exp(lp2) + exp(lp3))
  p1 <- 1/den
  p2 <- exp(lp2)/den
  p3 <- exp(lp3)/den
  P <- cbind(p1, p2, p3)
  
  rout <- replicate(n = 500, expr = {
    y <- apply(P, MARGIN = 1, function(x)sample(x = 1:3, size = 1, prob = x))
    d <- data.frame(y = factor(y), x, g)
    m <- multinom(y ~ x + g, data = d, trace = FALSE)
    aod <- car::Anova(m)
    aod["g", "Pr(>Chisq)"] < 0.05
    })
  # Calculate mean excluding NA values
  mean(rout, na.rm = TRUE)
}


sizes <- sapply(c(500, 700, 1000, 1500, 2000), sim_mod)

sizes
```

```{r claude code}
# Install and load required libraries
library(simr)
library(nnet)
library(mvtnorm)
library(MASS)
library(pwr)

# Comprehensive sample size calculation function
calculateSampleSize <- function(
  numPredictors = 17,
  minHazardRatio = 2.39,
  desiredPower = 0.8,
  alpha = 0.05,
  nsim = 200
) {
  # Effect size generation function
  generateEffectSizes <- function() {
    set.seed(123)
    
    # Generate effect sizes based on minimum hazard ratio
    # Convert hazard ratio to log odds ratio
    logOR <- log(minHazardRatio)
    
    # Create intercepts
    intercepts <- rnorm(numPredictors, 0, 0.5)
    
    # Create betas with specified effect size
    # Distribute effect sizes across predictors with some variability
    betas <- rep(logOR, numPredictors) * 
             rnorm(numPredictors, 1, 0.2)
    
    return(list(
      intercepts = intercepts,
      betas = betas
    ))
  }
  
  # Data generation function
  generateMultinomialData <- function(n, effects) {
    # Generate correlated predictors
    # Create correlation structure
    sigma <- diag(numPredictors) * 0.3 + 0.7
    X <- mvrnorm(n, mu = rep(0, numPredictors), Sigma = sigma)
    
    # Standardize predictors
    X_scaled <- scale(X)
    
    # Create linear predictors
    # Assuming 3-category outcome
    num_categories <- 3
    eta <- matrix(0, nrow = n, ncol = num_categories - 1)
    
    for (j in 1:(num_categories - 1)) {
      # Intercept + weighted predictors
      eta[, j] <- effects$intercepts[j] + 
                  X_scaled %*% (effects$betas * runif(numPredictors, 0.8, 1.2))
    }
    
    # Convert to probabilities
    exp_eta <- exp(eta)
    prob_denom <- 1 + rowSums(exp_eta)
    probs <- cbind(1 / prob_denom, 
                   exp_eta / rep(prob_denom, each = num_categories - 1))
    
    # Generate multinomial outcome
    outcome <- factor(apply(probs, 1, function(p) {
      sample(1:num_categories, size = 1, prob = p)
    }))
    
    return(data.frame(X_scaled, outcome = outcome))
  }
  
  # Power calculation function
  calculatePower <- function(n, effects) {
    # Simulate and analyze power
    powers <- replicate(nsim, {
      # Generate data
      data <- generateMultinomialData(n, effects)
      
      # Prepare formula dynamically
      predictors <- paste0("X", 1:numPredictors, collapse = " + ")
      formula <- as.formula(paste("outcome ~", predictors))
      
      # Fit multinomial model
      tryCatch({
        model <- multinom(formula, data = data, trace = FALSE)
        
        # Extract p-values
        z <- summary(model)$coefficients / summary(model)$standard.errors
        p_values <- 2 * (1 - pnorm(abs(z)))
        
        # Check if significant predictors meet threshold
        # Strict criteria: at least 80% of predictors significant
        sum(p_values[, -1] < alpha) / (numPredictors * (2)) >= 0.8
      }, error = function(e) FALSE)
    })
    
    # Calculate power
    mean(powers)
  }
  
  # Detailed sample size search function
  findSampleSize <- function(
    minN = 100, 
    maxN = 5000, 
    stepSize = 50
  ) {
    # Initialize tracking variables
    results <- data.frame(
      SampleSize = integer(),
      Power = numeric(),
      stringsAsFactors = FALSE
    )
    
    # Effect sizes
    effects <- generateEffectSizes()
    
    # Sample size search
    currentN <- minN
    bestResult <- NULL
    
    while (currentN <= maxN) {
      # Calculate power
      power <- calculatePower(currentN, effects)
      
      # Store results
      results <- rbind(results, data.frame(
        SampleSize = currentN,
        Power = power
      ))
      
      # Print intermediate results
      cat("Sample Size:", currentN, 
          "Power:", sprintf("%.4f", power), "\n")
      
      # Update best result if power meets threshold
      if (power >= desiredPower && 
          (is.null(bestResult) || currentN < bestResult$SampleSize)) {
        bestResult <- list(
          SampleSize = currentN,
          Power = power
        )
      }
      
      # Increment sample size
      currentN <- currentN + stepSize
      
      # Early stopping if power consistently low
      if (nrow(results) > 5) {
        recent_powers <- tail(results$Power, 5)
        if (all(recent_powers < 0.3)) break
      }
    }
    
    return(list(
      Results = results,
      BestResult = bestResult
    ))
  }
  
  # Run sample size calculation
  sampleSizeResults <- findSampleSize()
  
  return(sampleSizeResults)
}

# Run sample size calculation
set.seed(456)
results <- calculateSampleSize(
  numPredictors = 15,
  minHazardRatio = 2.39,
  desiredPower = 0.8,
  nsim = 200
)

# Visualization
library(ggplot2)

# Plot power curve
if (!is.null(results$Results)) {
  power_plot <- ggplot(results$Results, aes(x = SampleSize, y = Power)) +
    geom_line(color = "blue") +
    geom_point(color = "red") +
    geom_hline(yintercept = 0.8, color = "green", linetype = "dashed") +
    theme_minimal() +
    labs(
      title = "Power Analysis for Multinomial Logistic Regression",
      subtitle = paste("Minimum Hazard Ratio:", 2.39),
      x = "Sample Size",
      y = "Achieved Power"
    )
  
  # Print plot
  print(power_plot)
}

# Print detailed results
cat("\nBest Sample Size Results:\n")
if (!is.null(results$BestResult)) {
  cat("Recommended Sample Size:", results$BestResult$SampleSize, "\n")
  cat("Achieved Power:", sprintf("%.4f", results$BestResult$Power), "\n")
} else {
  cat("No sample size found meeting power criteria\n")
}

# Additional output of full results
print(results$Results)
```


```{r deepseek code}

# Load necessary libraries
library(nnet)  # For multinomial logistic regression

# Define parameters
k <- 3  # Number of categories in the outcome
alpha <- 0.05  # Significance level
power <- 0.8  # Desired power
effect_size <- 2.39 # Odds ratio (example)

# Function to simulate data and fit model
simulate_power <- function(n, effect_size, k, alpha) {
  # Simulate data
  X <- rnorm(n)  # Predictor variable
  beta <- log(effect_size)  # Log odds ratio
  logits <- cbind(0, beta * X)  # Logits for each category
  probs <- exp(logits) / rowSums(exp(logits))  # Probabilities
  y <- apply(probs, 1, function(p) sample(1:k, size = 1, prob = p))  # Outcome variable
  
  # Fit multinomial logistic regression
  model <- multinom(factor(y) ~ X)
  
  # Test significance of the predictor
  p_value <- summary(model)$coefficients[2, "Pr(>|z|)"]
  
  # Return whether the effect is significant
  return(p_value < alpha)
}

# Function to estimate power
estimate_power <- function(n, effect_size, k, alpha, n_sim = 1000) {
  power_est <- mean(replicate(n_sim, simulate_power(n, effect_size, k, alpha)))
  return(power_est)
}

# Find sample size
sample_size <- 100  # Initial guess
while (estimate_power(sample_size, effect_size, k, alpha) < power) {
  sample_size <- sample_size + 10
}

# Print the required sample size
print(paste("Required sample size:", sample_size))



```



## Logistic regression

```{r}

# install.packages("pwrss")
library(pwrss)

pwrss.z.logreg(p0 = 0.15, odds.ratio = 0.86, r2.other.x = 0.20,
               alpha = 0.05, power = 0.80,
               dist = "normal")

pwrss.z.logreg(p0 = 0.15, odds.ratio = 3.14, r2.other.x = 0.20,
               alpha = 0.05, power = 0.80,
               dist = "normal")

```



```{r}

library(WebPower)


wp.logistic(n = NULL, p0 = 0.15, p1 = 0.1, alpha = 0.05,
power = 0.8, family = "normal", parameter = c(0,1))



```

### use simr

```{r}
# Load necessary packages
library(simr) 

# Define your logistic regression model with desired effect sizes 
model <- lmer(binary_outcome ~ predictor + (1|grouping_factor), family = binomial(link = "logit"), data = data) 

# Simulate power analysis with 'powerSim'
power_result <- powerSim(model, nsim = 100, test = fcompare(binary_outcome ~ predictor)) 

# Interpret results
print(power_result) 
# This will show the estimated power for your model based on the simulated data

```

### G power

## Latent growth mixture models
## Reference: https://moshagen.github.io/semPower/#powerLGCM


```{r}

library(semPower)
nwaves = 3
powerLGCM <- semPower.powerLGCM(
  # define type of power analysis
  type = 'a-priori', alpha = .05, power = .8,
  # define hypothesis 
  nWaves = nwaves,
  means = c(.5, .2),     # i, s
  variances = c(1, .5),  # i, s
  covariances = .25,
  nullEffect = 'sMean = 0',
  # define measurement model
  nIndicator = rep(3, nwaves), loadM = .5
)
summary(powerLGCM)
```


## Survival analysis
### reference: https://cran.r-project.org/web/packages/powerSurvEpi/powerSurvEpi.pdf

```{r}

library(powerSurvEpi)

ssizeEpi.default(power = 0.80,
theta = 1.32, # asummed hazard ratio
p = 0.05, # proportion of subjects taking value one for the covariate of interest. 
psi = 0.5369, # proportion of subjects died of the disease of interest. 
rho2 = 0.251^2, # square of the correlation between the covariate of interest and the other covariate.
alpha = 0.05)


```

## ANOVA on G power

